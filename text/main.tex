\documentclass[11pt,english,a4paper,hidelinks]{book}
\input{settings}

% Add bibliography configuration
\addbibresource{bibliography.bib}

\begin{document}
\renewcommand{\listtablename}{List of Tables} 
\renewcommand{\tablename}{Table} 

% ############################### COVER PAGE #######################
\input{cover} 
% ############################### ABSTRACT ###############################
\newpage
\thispagestyle{empty}\ \\
\newpage
\thispagestyle{empty}

\section*{Abstract}
\noindent This study investigates the effectiveness of a factor-investing methodology developed by Tweenvest, leveraging a proprietary algorithm grounded in fundamental financial analysis. The algorithm scores companies across four key factors: Quality, Growth, Value, and Dividends.

\vspace{0.5cm}

\noindent The research aims to evaluate the profitability of investment strategies based on these scores over multiple profitability horizons from 1 month to five years. A comprehensive dataset was constructed, integrating factor scores with additional variables such as sector and geographic region, standardized for currency and timeframes.

\vspace{0.5cm}

\noindent Statistical analyses will explore relationships between these factors and returns, identifying optimal investment periods. Subsequently, predictive modeling—including econometric regressions, time series models, and neural networks—will be applied to assess the translation of these factors into market performance.

\vspace{0.5cm}

\noindent The study incorporates an interdisciplinary approach, combining financial theory and econometrics with advanced programming, data engineering, and artificial intelligence. This integration bridges Business Management and Telecommunications Engineering, offering insights into the practical application of Tweenvest's scoring algorithm and contributing to the advancement of financial technology analytics.


% ############################### DEDICATION #######################
\newpage
\thispagestyle{empty}
\vfill
\begin{flushright}
To my parents.
\end{flushright}
\vfill\vfill

% ############################### TABLE OF CONTENTS #######################
\newpage
\thispagestyle{empty}
\ \\
\addtocontents{toc}{\protect\thispagestyle{empty}}
\addtocontents{toc}{\protect\pagestyle{empty}}
\tableofcontents
\newpage

% ############################### LIST OF FIGURES AND TABLES #######################
\addtocontents{lof}{\protect\thispagestyle{empty}}
\addtocontents{lof}{\protect\pagestyle{empty}}
\listoffigures
\newpage

\addtocontents{lot}{\protect\thispagestyle{empty}}
\addtocontents{lot}{\protect\pagestyle{empty}}
\listoftables
\newpage

% ############################### ACRONYMS / CAPITALIZED TERMS #######################
\input{capitals}

% ############################### START OF CONTENT #######################
\clearpage
\pagenumbering{arabic}
\setcounter{page}{1}
\addtocontents{toc}{\protect\thispagestyle{empty}}

\part{Main Report}

% ############################### INTRODUCTION #######################
\chapter{Introduction to Fundamental Analysis}
\section{Definition}

\noindent Fundamental Analysis is a methodology used to evaluate the intrinsic value of a company, asset, or market by analyzing various economic, financial, and qualitative and quantitative factors. Unlike technical analysis, which focuses on price movements and chart patterns, fundamental analysis seeks to determine an asset's "true value" to identify investment opportunities that may be undervalued or overvalued in the market.
\vspace{0.5cm}

\noindent This intrinsic value is defined by numerous experts and renowned investors, including Benjamin Graham, Warren Buffett, and Pat Dorsey [II]; and many Stock's Index such as IBEX-35 or MSCI, as a company's ability to adapt to an ever-changing environment while adding value to the market and establishing barriers to entry for competitors—commonly known as economic moats.
\vspace{0.5cm}

\noindent Measuring these moats is challenging due to the difficulty of quantifying certain variables, such as brand strength and market influence. However, over the long term, these intangible factors translate into tangible financial data, reflected in a company's balance sheet, income statement, and cash flow statement; that when exposed to the public market share creates a need to buy or sell the stocks, altering the companies profitability. So from now on, this economic moats will be called \textit{alpha}, following the common literature. 

\section{Tweenvest's Scores}
Following this approach, Tweenvest developed its four main scores based on key focus areas that investors have to consider before making any decision:
\begin{itemize}
    \item Profitability
    \item Financial Health
    \item Predictability
    \item Consistent Growth
    \item Entrance Moment
    \item Dividends Payed
\end{itemize}

\subsection{Quality}

\noindent This Tweenvest's score is approached in a similar way that many successful investors would, distinguishing on three main categories: \textbf{profitability}, \textbf{financial health}, and \textbf{predictability}. Each of them includes inside of them multiple financial ratios that take account of different relevant data within the company's reports.

\vspace{0.5cm}
\noindent To understand the complexity of this score, we need to look at each category separately. Starting with \textbf{profitability} we can separate:

\subsubsection{Profitability Margins}
These are essential for understanding how a company is managing its costs and generating profits from its revenues.
\begin{itemize}
    \item \textbf{Net margin}: Represents net profit as a percentage of total sales, indicates a company's efficiency in generating profits after accounting for all expenses, taxes, and costs. A high net margin suggests that the company has good cost control and strong pricing power in the market.
    
    \item \textbf{Operating margin}: Measures operating profit (EBIT) as a percentage of total sales, providing a clear view of the profitability of a company's core operations, excluding interest and taxes. This metric is fundamental for evaluating a company's operational efficiency.
    
    \item \textbf{EBITDA margin}: Removes the effects of capital structure and accounting policies, offering a clear view of the company's pure operational profitability.
    
    \item \textbf{Gross margin}: Focuses on revenues after deducting the cost of goods sold, is a key measure of production efficiency and a company's ability to manage its direct costs.
\end{itemize}

\subsubsection{Performance Ratios}
These are used to measure the overall performance of the company.
\begin{itemize}
    \item \textbf{ROA (Return on Assets)}: Measures how efficiently a company converts its assets into profits. This is especially important in capital-intensive sectors, where efficient asset management can make a significant difference in profitability.
    
    \item \textbf{ROE (Return on Equity)}: Focuses on the profits generated per dollar of equity invested by shareholders. This ratio is crucial for evaluating a company's overall profitability from the shareholders' perspective. It is an especially valuable metric for investors seeking to maximize their returns on equity investment.
    
    \item \textbf{ROIC (Return on Invested Capital)}: Focuses on the return generated by all the funds invested in the company, including both shareholders' equity and debt. It is a comprehensive measure of a company's ability to generate value from all its sources of financing.
    
    \item \textbf{ROCE (Return on Capital Employed)}: Measures the funds used to finance operations, regardless of the source. This ratio is useful for comparing the efficiency of companies with different capital structures, as it focuses on total capital employed rather than just equity.
\end{itemize}

\noindent Also, to not only look at actives Tweenvest uses the cash generated to calculate: \textbf{CROIC} (Cash Return on Invested Capital), \textbf{CROCE} (Cash Return on Capital Employed), \textbf{OCF/Sales}, and \textbf{FCF/Sales}. And lastly it takes account also the Owner's income to calculate: \textbf{Owner's Income/Sales}, \textbf{Owner's CROIC} and \textbf{Owner's CROCE}.

\vspace{0.5cm}
\noindent Continuing with the financial health, we need to analyze the company's debt in multiple aspects:

\subsubsection{Leverage Ratios}
These ratios assess how much a company relies on debt to finance its assets and operations. They are essential for evaluating financial risk and long-term solvency.
\begin{itemize}
    \item \textbf{Financial Leverage} (Total Assets / Equity): Measures the proportion of a company's assets that are financed by shareholder equity. A higher ratio suggests the company is using more debt relative to equity, indicating greater financial risk but also potential return amplification through leverage.
    
    \item \textbf{Total Debt/Assets}: Indicates what portion of the company's assets is financed through debt. A lower ratio implies a more conservative capital structure, while a higher one may indicate increased risk if the company becomes over-leveraged.
    
    \item \textbf{Total Debt/Capital}: Measures the share of total capital (debt + equity) that comes from debt. This ratio is useful for understanding how dependent the company is on borrowed funds compared to its overall capital base.
    
    \item \textbf{Total Debt/Equity}: Compares the company's total debt to its shareholder equity. It provides insight into the balance between debt and equity financing. A high ratio may signal financial risk, but also the potential for higher returns if debt is managed well.
\end{itemize}

\subsubsection{Debt Coverage Ratios}
These metrics evaluate a company's ability to cover its debt using its earnings or cash flow. They reflect the sustainability of a company's debt in relation to its operational performance.

\begin{itemize}
    \item \textbf{Net Debt/EBIT}: Shows how many years it would take for a company to repay its net debt using EBIT (Earnings Before Interest and Taxes).
    
    \item \textbf{Net Debt/EBITDA}: Similar to the above, but adds back depreciation and amortization. This gives a more cash-focused view of a company's ability to handle its debt load, and is especially useful for comparing companies in capital-intensive industries.
    
    \item \textbf{Net Debt/FCF}: Evaluates how many years of free cash flow would be needed to pay off net debt. Since FCF includes investment needs, this ratio gives a more conservative view of debt sustainability.
    
    \item \textbf{Net Debt/Owner's Income}: Compares net debt to the income available to equity holders (after all operating and investing costs).
\end{itemize}

\subsubsection{Interest Coverage Ratios}
These ratios measure how easily a company can meet its interest payments on outstanding debt — critical for assessing short-term debt service capability.

\begin{itemize}
    \item \textbf{EBIT/Interest}: Indicates how many times a company can cover its interest expenses with its operating income.
    
    \item \textbf{EBITDA/Interest}: Similar to the above, but adds back depreciation and amortization. This gives a clearer picture of available cash earnings before fixed financial obligations, ideal for heavily asset-based businesses.
    
    \item \textbf{FCF/Interest}: Since FCF considers investment needs, this is a stringent test of how much real, discretionary cash is available for debt servicing.
    
    \item \textbf{Owner Earnings/Interest}: Evaluates a company's ability to meet interest payments based on the earnings effectively attributable to shareholders. It accounts for operational cash flow minus necessary capital expenditures.
\end{itemize}

\subsubsection{Liquidity Ratios}
These ratios measure a company's ability to meet short-term obligations with its short-term assets. They are essential for evaluating near-term financial health and risk of insolvency.

\begin{itemize}
    \item \textbf{Current Ratio} (Current Assets / Current Liabilities): Shows whether a company has enough assets to cover its short-term liabilities. A value above 1 is generally considered healthy, though excessively high values may imply inefficiency.
    
    \item \textbf{Quick Ratio} ((Current Assets - Inventory) / Current Liabilities): A more stringent version of the current ratio that excludes inventory, which may not be easily liquidated. It's useful in assessing true short-term liquidity.
    
    \item \textbf{Cash Ratio} (Cash and Equivalents / Current Liabilities): The most conservative liquidity metric, focusing only on cash and equivalents. It shows the immediate solvency of a company in a worst-case scenario.
    
    \item \textbf{OCF Ratio} (Operating Cash Flow / Current Liabilities): Assesses how well the company's operational cash flows can cover its current obligations. This offers a realistic view of liquidity since it's based on actual cash generation rather than accounting figures.
\end{itemize}

\subsubsection{Predictability}

\noindent And finally, for the quality score we need to look at the company's predictability. This is achieved by trying to fit values related to the company's success —such as Sales— to a exponential curve, which is supported by large financial literature.

\vspace{0.5cm}
\noindent After calculating all of this ratios, Tweenvest compares them to the sectors' median and interpolates it to create a single score for each ratio and then aggregates them all using personalized weights to create the final Quality Score, which is then showed to the clients:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/tweenvest/quality score.png}
    \caption{Tweenvest's Quality Score}
    \label{fig:quality_score}
\end{figure}

\subsection{Growth}
\noindent The Growth Score evaluates a company's historical growth across multiple key metrics, comparing them to industry standards. This comprehensive approach ensures a balanced assessment of growth across different aspects of the business.

\subsubsection{Revenue \& Profitability Growth}
\begin{itemize}
    \item \textbf{Sales}: Measures growth in core revenue streams
    \item \textbf{EBITDA}: Captures growth in operational cash-generating ability before non-cash and financing impacts
    \item \textbf{Operating Income}: Reflects growth in profit from core business operations
    \item \textbf{Net Income}: Includes all income sources, showing overall profitability growth
\end{itemize}

\subsubsection{Cash Flow Growth}
\begin{itemize}
    \item \textbf{Operating Cash Flow}: Measures growth in cash generation from operations
    \item \textbf{Simple FCF}: A straightforward proxy for available cash after essential investments
    \item \textbf{Levered/Unlevered FCF}: Provide detailed views of free cash flow with and without debt impact
    \item \textbf{Owner Earnings}: Useful for volatile capex cases, emphasizing cash available to shareholders
\end{itemize}

\subsubsection{Capital Base Expansion}
\begin{itemize}
    \item \textbf{Total Assets}: Indicates expansion in overall asset base
    \item \textbf{Equity}: Reflects growth in shareholders' claim on the business
    \item \textbf{Tangible Book Value}: Highlights growth in physical net assets, excluding intangibles
    \item \textbf{Invested Capital}: Captures total capital being put to productive use
    \item \textbf{Capital Employed}: A broader measure of capital supporting business operations
\end{itemize}

\subsubsection{Per-Share Value Growth}
\begin{itemize}
    \item \textbf{Diluted EPS}: Tracks per-share earnings growth, accounting for dilution effects
    \item \textbf{Diluted Shares}: Included to track share count changes, ensuring EPS growth isn't artificially inflated by buybacks or dilution
    \item \textbf{Ordinary DPS}: Tracks the growth of shareholder payouts, a proxy for confidence in future earnings
\end{itemize}

\vspace{0.5cm}
\noindent To compute the Growth Score, Tweenvest calculates 10-year, 5-year, and 3-year averages and then interpolates the growth rate to industry standards. This approach reinforces the long-term investment philosophy, giving lasting growing companies a better score.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/tweenvest/growth score.png}
    \caption{Tweenvest's Growth Score}
    \label{fig:growth_score}
\end{figure}

\subsection{Valuation}
\noindent The Valuation Score measures how attractively a company is priced relative to its fundamentals such as earnings, cash flow, sales, and dividends. This is critical for investors following value investing principles, where the goal is to buy quality companies for less than their intrinsic worth. The algorithm evaluates a series of valuation multiples—both price-based and enterprise value-based—and dividend yield.

\subsubsection{Price-Based Multiples}
\begin{itemize}
    \item \textbf{P/E} (Market Cap / Adjusted TTM Earnings): Measures how much investors are willing to pay per dollar of earnings
    \item \textbf{P/S} (Market Cap / TTM Revenue): Useful when earnings are volatile; shows valuation relative to sales
    \item \textbf{P/CF} (Market Cap / TTM Operating Cash Flow): Reflects valuation relative to cash-generating ability
    \item \textbf{P/B} (Market Cap / Tangible Equity): Especially relevant for asset-heavy sectors like banks or industrials
\end{itemize}

\subsubsection{Enterprise Value-Based Multiples}
\begin{itemize}
    \item \textbf{EV/Sales} (Enterprise Value / TTM Revenue)
    \item \textbf{EV/EBITDA} (Enterprise Value / TTM EBITDA)
    \item \textbf{EV/EBIT} (Enterprise Value / TTM EBIT)
    \item \textbf{EV/FCF} (Enterprise Value / TTM Free Cash Flow)
\end{itemize}

\subsubsection{Yield-Based Valuation}
\begin{itemize}
    \item \textbf{Dividend Yield (\%)} (Dividend per Share / Price per Share)
\end{itemize}

\vspace{0.5cm}
\noindent Each of these ratios is compared to multiple historical statistics and sectoral benchmarks to create individual scores and then average them.

\vspace{0.5cm}
\noindent \textbf{Note:}
\begin{itemize}
    \item Market Cap = Share Price × Total Outstanding Shares
    \item Enterprise Value = Market Capitalization + Total Debt - Cash + Marketable Securities
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/tweenvest/value score.png}
    \caption{Tweenvest's Value Score}
    \label{fig:valuation_score}
\end{figure}


\subsection{Dividend}
\noindent When analyzing Tweenvest most common investors profile, we see a high tendency to dividend investment.

\vspace{0.5cm}
\noindent The Dividend Score measures the attractiveness, reliability, and growth potential of a company's dividend payments. It helps investors assess whether the dividend is both rewarding today and sustainable for tomorrow—a key concern for many users of the platform that are income-focused and long-term investors using mainly dividends as their principal concern. And it is built from three primary components:

\subsubsection{Safety}
\begin{itemize}
    \item \textbf{Payout Ratio EPS} (DPS / Diluted EPS): Shows if dividends are covered by accounting earnings
    \item \textbf{Payout Ratio FCF} (DPS / Free Cash Flow per Share): Shows if dividends are funded by real cash generation
    \item \textbf{Payout Ratio Owner Earnings} (DPS / Owner Earnings per Share): A conservative test of sustainability (excludes CAPEX)
\end{itemize}

\subsubsection{Growth}
\noindent Measures how consistently and strongly the dividend has grown over time.
\begin{itemize}
    \item \textbf{Ordinary DPS CAGR}: 3-Year, 5-Year, 10-Year growth rates
\end{itemize}

\subsubsection{Yield}
\noindent This evaluates the attractiveness of the dividend today relative to the company's historical averages and sector benchmarks.
\begin{itemize}
    \item \textbf{Dividend Yield} (DPS / Price per Share): Represents how much income an investor receives annually from dividends
\end{itemize}

\vspace{0.5cm}
\noindent These components are individually scored weighted and interpolated against industry benchmarks to form a composite score. Finally, the algorithm adjusts this score based on how many years the dividend has been maintained or increased, rewarding consistency.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/tweenvest/dividend score.png}
    \caption{Tweenvest's Dividend Score}
    \label{fig:dividend_score}
\end{figure}


\section{Problems with the current approach}

\noindent In the context of this thesis, we have to explain the need to make a systematic review of the scores to check wether the simplifications and hypotheses assumed by the financial consensus truly show a company's ability to create \textit{alpha} over time or not.


\subsection{Unquantified Variables}
\noindent Since the current model only includes variables found in a company's financial statements, there is a significant amount of relevant information being left out, for example:

\begin{itemize}
    \item The \textbf{perceived differentiation of a product} is one such element that may not be properly captured by traditional financial analysis. A trusted brand or strong reputation can allow a company to charge premium prices and build customer loyalty, generating extraordinary long-term profits. For instance, brands like Tiffany or Rolex have high perceived value that justifies elevated prices—something that cannot be easily quantified using standard financial metrics such as profit margins or return on capital.

    \item Additionally, strategies such as \textbf{cost leadership} and offering products at lower prices can provide a significant competitive advantage that is not always directly reflected in financial data.

    \item Companies may also establish \textbf{barriers to entry} and \textbf{high switching costs} for customers, making it more difficult for them to move to competitors. These strategies may involve investments in technology, patents, or simply building long-term customer and employees relationships.
\end{itemize}

\noindent To properly measure these variables, a more exhaustive analysis of each company would be required, pulling from multiple secondary information sources such as news articles, conference transcripts, customer blogs, competitive product reviews, and more. This task is far more difficult to automate via code, as it would require multimodal AI techniques—thus, it will fall outside the scope of this work.

\subsection{Emergent Effects in Complex Systems}

\noindent In complex systems like financial markets, network effects emerge, making most relations between variables not be linear or stationary, adding noise to actual reliable data behind it:

\begin{itemize}
    \item \textbf{Herd behavior} is a network effect where investors tend to follow the actions of others rather than rely on their own analysis. This can amplify market movements, both upward and downward. Herd behavior can lead to speculative bubbles and abrupt corrections when collective expectations shift.

    \item \textbf{Feedback loops} are another network effect where market participants' actions reinforce existing market behavior. For example, rising asset prices may attract more investors, which in turn drives prices even higher. This type of positive feedback can cause price escalation that becomes detached from underlying fundamentals. Conversely, negative feedback can occur during a sell-off, where falling prices trigger more selling, further accelerating the decline.
    \item \textbf{Macro-level influences} are another crucial factor where broader structural changes—such as political decisions, economic policies, technological shifts, or industry-wide trends—can significantly impact market behavior. These larger forces often operate beyond the scope of individual company analysis and can create ripple effects throughout the entire market ecosystem.
\end{itemize}

\subsection{Linearity and Stationarity}

\subsubsection{Non-linearity}
\noindent The relationships between financial variables are often non-linear, meaning that changes in one variable may not result in proportional changes in another. For example:

\begin{itemize}
    \item \textbf{Economies of scale} can create non-linear relationships between production volume and costs. As a company grows, it may experience decreasing marginal costs due to better resource utilization, bulk purchasing discounts, or spreading fixed costs over larger output.
    
    \item \textbf{Market saturation} can lead to diminishing returns on marketing spend or R\&D investments. Initial investments might yield significant returns, but as the market becomes saturated, additional spending may produce smaller incremental benefits.
    
    \item \textbf{Competitive dynamics} can create threshold effects where small changes in market share or pricing can trigger significant shifts in competitive position or profitability.
\end{itemize}

\subsubsection{Trends}

\noindent Traditionally, standard growth ratios have been used, based on the assumption that in competitive markets, when a new business model or opportunity emerges with above-average margins, entrepreneurs quickly move in to capitalize on it—eventually saturating the opportunity and driving margins back down to average levels over time. But what happens when there is a significant shift in trend? 

\vspace{0.5cm}
\noindent Markets are "fluctuating entities", so static metrics can become problematic when underlying trends change. A clear example is the rise of artificial intelligence and the surge in stock prices of companies involved in the production and development of the necessary technologies.



% ############################### OBJECTIVES #######################
\chapter{Objectives}

\noindent The main objective of this thesis is to check if the scores actually represent an objective and accurate view of the company's ability to generate \textit{alpha} for different time frames, and propose changes to the current algorithm if needed. To do this, we need to follow multiple steps:

\begin{enumerate}
  \item \textbf{System Architecture Enhancement}: Modify Tweenvest's database architecture to enable historical score storage and retrieval and create the datasets for the analysis.
  \item \textbf{Data Curation}: Clean and preprocess the collected data to ensure consistency and reliability, handling missing values and outliers appropriately.
  \item \textbf{Exploratory Analysis}: Process and analyze the data to check if the distributions and correlations between variables, looking for early on patterns to later compare and use for the modeling.
  \item \textbf{Predictive Modeling}: Develop a multiple predictive models to check what is the best way to use the scores for multiple investment strategies.
  \item \textbf{Validation \& Benchmarking}: Back-test the algorithms to check their performance and compare them with S\&P 500 profitabilities.
\end{enumerate}


% ############################### METHODOLOGY #######################
\chapter{Methodology \& Theoretical Framework}

\section{Architecture and Database}
\subsection{Code practices}
Since the code that needs to be changed is used daily in a production environment by Tweenvest, it is important to follow some practices to ensure the code is easy to understand, maintain, and to avoid introducing new bugs.

\begin{itemize}
  \item \textbf{Understanding the code}: Before starting to work on the code, it is important to understand the codebase and the purpose of the code. For this, it is recommended to use some tools like flux diagrams, code comments, and documentation. As it will be shown later on.
  \item \textbf{Testing}: Every function and class should have unit tests that cover all possible scenarios for not introducing new bugs.
  \item \textbf{Logging}: After each feature is implemented, it is important to analyze the logs to check if the feature is working as expected, and to see if there are any possible optimizations to be made in the queries to improve the performance.
  \item \textbf{Code review}: Whenever all of this is done, the code needs to be reviewed and approved by the team to be merged into the main branch.
\end{itemize}

\subsection{Data creation}

%TODO

\subsection{Data storage}

%TODO

\section{Preprocessing}

%TODO

Once we have our raw dataset, we need to inspect it and make sure it is consistent and ready to be used for the modeling. This part is really important to avoid any bias in the modeling process and to only use the data that is actually relevant for the analysis.

\noindent 

% \vspace{0.5cm}
% \noindent As we will explain later on, many of the models follow the premises of normal distribution.

\subsection{Data Consistency}

Since all of our data is coming from a data provider that uses OCR as one of their tools whenever they don't have the data in a structured format, for this reason we need to be very restrictive with the data that we use. Also, as we mentioned before, some of our algorithms for calculating the scores use long term data such as 10 year growths rates. Which means that all of the companies that stopped existing or that were acquired by other companies will create null scores. 

\vspace{0.5cm}
\noindent This is one of the biggest \textbf{limitations of the current approach}: we will be missing the data of companies that stopped existing or that were acquired by other companies, if they didn't exist for 10 or more years.

\vspace{0.5cm}
\noindent Also, we chose to set all of the Dividend null scores to 0, since this score is null whenever the company doesn't pay dividends. We need to check if this score is relevant for long term profitability, so we can't exclude it from the analysis whenever it's null.


% ############################### PREPROCESSING #######################
\subsection{Data Transformation}

To ensure a robust and well-performing model, it is often necessary to preprocess the data by transforming it into a format more suitable for learning algorithms—while preserving the ability to revert it back to its original scale if needed. In many practical cases, rather than analyzing the precise distribution of each variable, we simply standardize the data by centering each feature (subtracting the mean) and scaling it (dividing by the standard deviation), provided the feature is not constant.

\vspace{0.5cm}
\noindent This step is particularly important because many components of machine learning models—such as the RBF kernel in Support Vector Machines or the regularization terms (L1 and L2) in linear models—implicitly assume that features are centered around zero and have comparable variances. Without this adjustment, features with significantly larger variances could dominate the optimization process, leading the model to underweight or ignore more informative but lower-variance features.

\subsubsection{Standard Scaler}
When training the neural networks, it is really important to standardize the data. We can easily use the StandardScaler from the scikit-learn library that uses the transformation of a feature \(x_i\) is calculated as:

\begin{equation}
z_i = \frac{x_i - \mu}{\sigma}
\end{equation}

\noindent where \(\mu\) is the mean of the feature and \(\sigma\) is its standard deviation. This transformation results in a distribution with a mean of 0 and a standard deviation of 1.



\subsubsection{Gaussian Transformation}

In numerous modeling applications, having normally distributed features is advantageous. Power transformations represent a set of parametric, monotonic functions that are an extension of the Box-Cox transformation. They are designed to convert data from various distributions into approximately Gaussian distributions, thereby reducing variance fluctuations and decreasing distribution asymmetry. These family of transformations are also reversible, so we can easily transform the data back to its original space when needed.

\vspace{0.5cm}
\noindent We finally decided to use the Yeo-Johnson transformation because it allows for negative values and it is reversible.

\begin{equation}
x_i^{(\lambda)} =
\begin{cases}
\frac{[(x_i + 1)^\lambda - 1]}{\lambda}, & \text{if } x_i \geq 0, \lambda \neq 0 \\
\ln(x_i + 1), & \text{if } x_i \geq 0, \lambda = 0 \\
-\frac{[(-x_i + 1)^{2 - \lambda} - 1]}{2 - \lambda}, & \text{if } x_i < 0, \lambda \neq 2 \\
-\ln(-x_i + 1), & \text{if } x_i < 0, \lambda = 2
\end{cases}
\end{equation}

\noindent Where \(\lambda\) is a power parameter that helps minimize the skewness of the data. And since we have already deleted extreme outliers, the final distribution shouldn't be too distorted.

\vspace{0.5cm}

\noindent Here are some exammples of the transformations for different distributions:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/code/transformations/yeo-johnson.png}
    \caption{Yeo-Johnson Transformations examples}
    \label{fig:yeo-johnson}
\end{figure}


\section{Outlier Detection}

Anomalies are data patterns that have different data characteristics from normal
instances. The ability to detect anomalies has significant relevance, since anomalies often provide critical and actionable information in many different contexts.

\vspace{0.5cm}
\noindent For example, anomalies in credit card transactions could signify fraudulent use of
credit cards. An unusual computer network traffic pattern could stand for an
unauthorised access.


\subsection{Inter Quartile Range}

Given a univariate dataset, the Interquartile Rule identifies outliers based on the interquartile range (IQR). The steps are as follows:

\begin{enumerate}
    \item Compute the first quartile ($Q_1$), which is the 25th percentile of the data.
    \item Compute the third quartile ($Q_3$), which is the 75th percentile of the data.
    \item Calculate the interquartile range:
    \[
        \text{IQR} = Q_3 - Q_1
    \]
    \item Define the lower and upper bounds for non-outlier values following the 1.5 rule:
    \[
        \text{Lower bound} = Q_1 - 1.5 \times \text{IQR}
    \]
    \[
        \text{Upper bound} = Q_3 + 1.5 \times \text{IQR}
    \]
    \item Any data point $x$ is considered an outlier if:
    \[
        x < Q_1 - 1.5 \times \text{IQR} \quad \text{or} \quad x > Q_3 + 1.5 \times \text{IQR}
    \]
\end{enumerate}

\noindent As we can see this is a very simple and intuitive method, but it has some limitations:

\begin{table}[h!]
    \centering
    \begin{tabular}{|p{7cm}|p{7cm}|}
    \hline
    \textbf{Advantages} & \textbf{Limitations} \\
    \hline
    Simple to compute and interpret. & Not suitable for multivariate data with correlated features. \\
    \hline
    Non-parametric. & May misclassify values in skewed distributions as outliers. \\
    \hline
    Robust to extreme values, since it relies on percentiles rather than the mean. & Fixed multiplier is arbitrary and may need tuning for each dataset. \\
    \hline
    \end{tabular}
    \caption{Advantages and Limitations of the Interquartile Rule}
    \end{table}
    

\subsection{Single Vector Machine}

\noindent Support Vector Machines (SVMs) represent a robust class of supervised learning algorithms that can be effectively adapted for anomaly detection tasks. This algorithm establishes itself on the premise that the majority of real-world data is inherently normal. Where the goal is to define a boundary encapsulating the normal instances in the feature space, thereby creating a region of familiarity. 

\vspace{0.5cm}
\noindent To capture the bases of the scikit-learn implementation \textcite{scikit2025oneclasssvm} used in this work, the principle idea is to find a sphere, of minimum volume, containing all the training samples. This sphere, described by its center c and its radius r, is obtained by solving the constrained optimization problem: \cite{zineb2012simple}
\begin{equation}
\min_{r,c} r^2 \quad \text{subject to} \quad \|\Phi(x_i) - c\|^2 \leq r^2 \quad \text{for} \quad i = 1, 2, \ldots, n
\end{equation}

\vspace{0.5cm}
\noindent This boundary is strategically positioned to maximize the margin around the normal data points, allowing for a clear delineation between what is considered ordinary and what may be deemed unusual. This emphasis on margin maximization is akin to creating a safety buffer around the normal instances, fortifying the model against the influence of potential outliers or anomalies.

\vspace{0.5cm}
\noindent In summary, this method has the following characteristics:

\begin{table}[H]
    \centering
    \begin{tabular}{|p{7cm}|p{7cm}|}
        \hline
        \textbf{Pros} & \textbf{Cons} \\
            \hline
            Effective for high-dimensional data and complex boundaries. & Sensitive to the choice of kernel and hyperparameters (e.g., $\nu$, $\gamma$). \\
            \hline
            Works well when the training set contains mostly or only normal instances. & Computationally intensive, especially on large datasets. \\
            \hline
            Can capture nonlinear patterns using kernels (e.g., RBF kernel). & Difficult to interpret or explain the decision function. \\
            \hline
            No need for labeled data from the outlier class. & -- \\
            \hline
            Solid theoretical foundation and widely supported in machine learning libraries. & -- \\
        \hline
    \end{tabular}
    \caption{Pros and Cons of One-Class SVM for Outlier Detection}
\end{table}
    
    
\subsection{Isolation Forest}

As we are seeing in this small fraction of outlier detection methodologies, most of the existing anomaly detection approaches are based on the premise of normal distributions, then identify anomalies as those that do not conform to the normal profile.

\noindent As it is explained in the original paper \textcite{liu2012isolation}, the Isolation Forest algorithm constructs multiple isolation binarytrees (iTrees) from the dataset, where anomalies are identified as data points that exhibit shorter average path lengths across these trees. The algorithm's configuration involves three key parameters:
\begin{itemize}
    \item The number of trees to generate
    \item The size of the subsample
    \item The maximum tree height during the assessment phase
\end{itemize}


\noindent Lets consider a dataset $X = \{x_1, \dots, x_n\}$ containing $n$ points in $d$-dimensional space, and a subset $X' \subset X$. An Isolation Tree (iTree) is constructed as follows:

\begin{itemize}
    \item Each node $T$ in the tree is either:
    \begin{itemize}
        \item An external node (leaf) with no children, or
        \item An internal node with exactly two children ($T_l$ and $T_r$) and a test condition
    \end{itemize}
    \item The test condition at each internal node consists of:
    \begin{itemize}
        \item A randomly selected attribute $q$
        \item A split value $p$
        \item A test $q < p$ that determines whether a point goes to $T_l$ or $T_r$
    \end{itemize}
\end{itemize}

\noindent The tree construction process recursively partitions $X'$ by randomly selecting attributes and split values until either:
\begin{itemize}
    \item The node contains only one instance, or
    \item All instances at the node have identical values
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/code/outliers/IF.jpeg}
    \caption{Visualization of how Isolation Forest works.}
    \label{fig:isolation_forest}
\end{figure}

\noindent As we can see in Figure \ref{fig:isolation_forest}, once the iTree is fully constructed, each point $x_i \in X$ is isolated at a leaf node. The path length $h(x_i)$ of a point $x_i$ is defined as the number of edges traversed from the root to its leaf node. Points with shorter path lengths are considered more likely to be anomalies, as they require fewer splits to be isolated from the rest of the data.

\noindent This method works very good in most cases, but it has also its own limitations

\begin{table}[H]
    \centering
    \begin{tabular}{|p{7cm}|p{7cm}|}
    \hline
    \textbf{Pros} & \textbf{Cons} \\
    \hline
    Efficient and scalable to large datasets due to its tree-based structure. & Less effective for detecting local outliers in densely clustered regions. \\
    \hline
    Non-parametric: does not assume any data distribution. & Performance can vary with the choice of parameters like number of estimators and subsample size. \\
    \hline
    Naturally handles high-dimensional data. & Not suitable for detecting subtle anomalies that closely resemble normal data. \\
    \hline
    Requires little preprocessing (no need for feature scaling). & Interpretation of results is less intuitive compared to statistical methods. \\
    \hline
    Robust to irrelevant features due to random sub-sampling. & May struggle with data that has complex, non-isolated outlier structures. \\
    \hline
    \end{tabular}
    \caption{Pros and Cons of the Isolation Forest Method for Outlier Detection}
\end{table}

\subsection{Local Outlier Factor}

Since we are looking at outliers from the perspective of correlations between the different response variables to see which companies don't behave naturally. We also checked on the Local Outlier Factor (LOF) for being based on hyper-plane densities of data.

\vspace{0.5cm}
\noindent The algorithm measures the local deviation of the density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood. More precisely, locality is given by k-nearest neighbors, whose distance is used to estimate the local density. By comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. These are considered outliers. \textcite{breunig2000lof}


\begin{table}[H]
    \centering
    \begin{tabular}{|p{7cm}|p{7cm}|}
    \hline
    \textbf{Pros} & \textbf{Cons} \\
    \hline
    Can detect outliers relative to their local neighborhood, allowing it to identify context-specific anomalies. & LOF scores are relative and lack a universal interpretation threshold for outliers. \\
    \hline
    Effective in datasets with varying densities — unlike global methods, it can recognize outliers near dense clusters. & Sensitivity to parameter selection (e.g., number of neighbors) can affect performance and consistency. \\
    \hline
    Works well across domains, such as network intrusion detection or classification tasks. & Interpretation of LOF scores can vary significantly between datasets or even within a dataset. \\
    \hline
    Applicable in any domain where a dissimilarity measure is defined, not limited to vector spaces. & Not inherently scalable to very large datasets without approximation or optimization. \\
    \hline
    Easily generalizable and adaptable for use in spatial data, temporal data, and network structures. & Difficult to determine a consistent threshold for what constitutes an outlier. \\
    \hline
    \end{tabular}
    \caption{Pros and Cons of the Local Outlier Factor (LOF) Method}
\end{table}
    
\subsection{Multi Criteria Outlier Detection}

There is a big controversy in the field about the best method to detect the outliers, since they can vary depending on the methodology used.

\vspace{0.5cm}
\noindent After reading some aggregative methods such as \textcite{abro2020stacking}, I decided to create a multi-modal method myself, that combines the results of the different methods to get a more robust result.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/code/outliers/multimodal.png}
    \caption{Multi Criteria Outlier Detection Method}
    \label{fig:multimodal}
\end{figure}


\vspace{0.5cm}
\noindent This method consists in homogenizing the variables with a common data transformation so it is Normal. To make sure that the final data isn't being transformed we copy the original dataset earlyer. Then we use multiple methods to detect the differente outliers and then we make the intersection to get the common outliers between all of them. After this process we have tagged the orinal dataset rows, assuring that the data that will be excluded is only formed by \textit{true outliers}.


% ############################### PREDICTIVE MODELS #######################
\section{Predictive Models}
The main objective of this thesis is to check if the scores have any relation with the future profits of the companies. For doing this in the most generalized way, instead of creating a portfolio following some subjective criteria on the scores, markets and other variables, we will use predictive modeling to check for the actual relationships between the response variables and the predictive ones.

\vspace{0.5cm}
\noindent As a first approach, we will be creating a set of models that separate the scores and the different returns. 

\subsubsection{Response Variables}
We will use different profits that from know one will be representad as \(P_i\) for each time horizon: \(i \in \{1 \text{ month}, 3 \text{ months}, 6 \text{ months}, 1 \text{ year}, 2 \text{ years}, 5 \text{ years}\}\)

\subsubsection{Explanatory Variables}
For being able to train the models, we have different variable categories:

\begin{itemize}
    \item \textbf{Main variables}, these are numerical variables from \textbf{Tweenvest's Scores}, that are in the range of 0 to 100: \(S_j\) where   \(j \in \{\text{ Quality; Value; Dividend; Growth}\}\)
    \item \textbf{Dummy variables}, these are boolean variables that allow us to separate the companies by:
    \begin{itemize}
        \item \textbf{Industry}: \(I_k\) where \(k \in \{\text{Financials; Healthcare; etc.}\}\)
        \item \textbf{Region}: \(R_l\) where \(l \in \{\text{Europe; North America; etc.}\}\)
    \end{itemize}
    \item \textbf{Market Variables}, these are numeric variables that represent the companies \textit{"size"} using the: \textbf{Market Cap} and \textbf{Volume}.
\end{itemize}

\subsection{Regression Models}

To create the econometrical models, we want to be able to understand the actual relationship between the response variables and the explanatory ones. For this we started with the creation of multiple regression models.

\subsubsection{Linear Regression}

As for the first models, we created a series of linear regression models for each factor score that
includes the interactions between the dummy variables and the score used in each model.

\begin{equation}
    \hat{P_i}=\beta_0+\beta_1 \cdot S_j + 
\sum_{n=2}^{N}\beta_{n}\cdot D_n + S_j \cdot \sum_{m=N+1}^{M}\beta_{m}\cdot D_m
\end{equation}

\noindent where:
\begin{itemize}
    \item \(P_i\) is the estimated profit for each time horizon \(i\)
    \item \(S_j\) is the score for each factor \(j\)
    \item \(D_{n,m}\) are the dummy variables for the industry and region, where \(D_{n,m} := I_k \cup R_l\)
    \item \(\beta\) are the coefficients that weight each variable.
\end{itemize}

\noindent For fitting the models, we will also apply a backward selection to remove the variables that are not statistically significant, using the \textbf{p-value} as the criterion.

\subsubsection{Generalized Additive Models}

Because the relationships between the variables are not linear, we also used the Generalized Additive Models (GAM) to check if they are able to capture the non-linear relationships between the variables.

\vspace{0.5cm}
\noindent According to \textcite{pygam2018}, GAMs are smooth semi-parametric models that can capture non-linear relationships between variables. They take the form:

\begin{equation}
    g(\mathbb{E}[y|X]) = \beta_0 + f_1(X_1) + f_2(X_2,X_3) + \dots + f_M(X_N)
\end{equation}

\noindent where:
\begin{itemize}
    \item \(X^T = [X_1, X_2, \dots, X_N]\) are the independent variables
    \item \(y\) is the dependent variable.
    \item \(g()\) is the link function that relates the predictor variables to the expected value of the dependent variable.
    \item \(f_i()\) are feature functions built using penalized B-splines, which automatically model non-linear relationships without requiring manual transformation of variables.
\end{itemize}

\noindent In our case, we will use pyGAMs LinearGAM since it gives a Normal error distribution, and an identity link.

\subsection{Time Series}
In the stock market, the prices of the stocks are not independent of each other, they are correlated in the time series and they also have memory on past behavior of the company. This is what is known as \textbf{Momentum}, so companies that have a good past performance are more likely to have a good future performance, and vice versa.

\vspace{0.5cm}
\noindent For this reason, we contemplated two possibilities:

\subsubsection{ARIMA Models}
To take account the profit tendency, we implemented a ARIMA model to improve the regression models performance by using the residues of the already fitted model:

\begin{equation}
    P_i = \hat{P_i} + {\varepsilon_i} \quad \longleftrightarrow \quad {\varepsilon_i} \sim ARIMA_i(p,d,q)
\end{equation}

\noindent The ARIMA model is defined as:

\begin{equation}
    \phi_i(B_i)(1 - B_i)^{d_i} \varepsilon_i^t = \theta_i(B_i) \eta_i^t
    \end{equation}
    
    \noindent where for each $i$ profit:
    \begin{itemize}
      \item $\varepsilon_i^t$ is the residual at time $t$.
      \item $\eta_i^t$ is a white noise error term (innovation).
      \item $B_i$ is the backshift operator, such that $B \varepsilon_i^t = \varepsilon_i^{t-1}$.
      \item $\phi_i(B) = 1 - \phi_{i1} B - \phi_{i2} B^2 - \dots - \phi_{ip} B^p$ is the autoregressive (AR) polynomial.
      \item $\theta_i(B) = 1 + \theta_{i1} B + \theta_{i2} B^2 + \dots + \theta_{iq} B^q$ is the moving average (MA) polynomial.
      \item $d_i$ is the order of differencing.
    \end{itemize}
    


\subsubsection{Windowed Models}
And for capturing possible memory of the companies behavior in different aspects, we also will implement a windowed model that will use the last \(n\) scores statistical properties to predict future profits.

\vspace{0.5cm}
\noindent So for each score \(j\), we will calculate the average and the standard deviation of the scores over the last \(w\) periods. Let \(S_j^t\) be the score at time \(t\), then for a window of size \(w\):

\begin{equation}
    \bar{S}_j^w = \frac{1}{w} \sum_{i=t-w+1}^{t} S_j^i
\end{equation}

\noindent where:
\begin{itemize}
    \item \(\bar{S}_j^w\) is the average score over window \(w\) for score \(j\)
    \item \(w\) can be 3 or 6 months, 1 or 2 years.
    \item \(t\) is the current time point
    \item \(j\) represents the different scores (Quality, Growth, Value, Dividends)
\end{itemize}

\noindent We will also calculate the standard deviation of the scores over the same window:

\begin{equation}
    \sigma_j^w = \sqrt{\frac{1}{w} \sum_{i=t-w+1}^{t} (S_j^i - \bar{S}_j^w)^2}
\end{equation}

\noindent These statistical measures will be used as additional features in our predictive models to capture the temporal behavior of each score.

\subsection{Neural Networks}

Finally, for trying to capture the non-linear relationships between all the available variables, including the windowed statistical properties, we will use the Neural Networks.



% ############################### DEVELOPMENT AND RESULTS #######################
\chapter{Development and Results}

\section{Creation of the Dataset}

\noindent We have understood the nature of the data, explained the bases of how the scores are calculated, and the code methodologies we will be using. Now we need to create a proper dataset.

\vspace{0.5cm}
% TODO: add the capitals for DB
\noindent At the current moment of beginning this work, Tweenvest kept a large DB (database) of most of the needed information for creating the dataset. This includes all of the price histories, historical currency multipliers to dollar, dividends payed per stock… but when it came to the scores we had a traceability problem.

\vspace{0.5cm}
\noindent For optimizing the costs and structure of the DB, Tweenvest chose to only save the latest score for each company and overwriting it each day when calculating the newest on, which led to the first main task.

\subsection{Updating Tweenvest Code}

After locating where the score calculators are called, we have to understand the code's architecture and the relationships between the different database models to see exactly what needs to be changed. Tweenvest uses Django as the main backend's technology to handle a relational database with PostgreSQL, this is due to the nature of the financial data where one the information is related to another.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/tweenvest/general architecture.png}
    \caption{General Tweenvest's Architecture}
    \label{fig:general_architecture}
\end{figure}

\noindent As we can see, there are two different actions occurring at the same time. For one side we have all of the requests coming from the users interactions, and for the other hand the programmed jobs that need to be executed every day to update all of the financial information of the stocks, or necessary actions such as sending emails to get authentication pins.

\vspace{0.5cm}
\noindent Since we are changing the data logic for calculating and storing historical data of the factor scores, we need to assure that the systems capabilities don't exceed the platforms needs, and that the changes don't affect the normal calculation of the factor scores, so we need to look at the code corresponding to the factor scores.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/tweenvest/scores schema.png}
    \caption{Scores Calculations Code Schema}
    \label{fig:general_architecture}
\end{figure}

\noindent With this schema we have simplified the logic behind several functions related to the calculations of each factor score. Quickly we realize that we have modify the queries to include an extra filter, \textit{calculation\_date} to get models related to the score at any time, this way we can set it as today whenever we are just calculating the current ones.

\vspace{0.5cm}
\noindent Following the propagation schema, the root change comes from the functions \textit{calculate\_scores}, \textit{get\_indexes\_information} and \textit{get\_quality\_ratios\_by\_index}, these are the ones used by the production cron jobs to calculate the factor scores daily so we set it to None for normal use and then added date as a secondary option. This helps us create very readable logic to adapt the filters to both scenarios, for example in the value score:

\begin{lstlisting}[language=Python]
if calculation_date is None:
    calculation_date = date.today()
    filters = {
        "stock": stock,
        "date__gte": calculation_date - relativedelta(years=10),
    }
else:
    filters = {
        "stock": stock,
        "date__gte": calculation_date - relativedelta(years=10),
        "date__lte": calculation_date,
    }
phs = (
    PriceHistory.objects.filter(**filters)
)
\end{lstlisting}

\noindent The main change for this part of the code was to add a date filter to get only data "lower than or equal" to the calculation date. This may seem a bad approximation, but it has to be done this way due to the nature of the data; most of the financial data come from annual, semestral, and quarterly reports, except to the price history that is updated daily.

\vspace{0.5cm}
\noindent Along the way of analyzing the code's structure we made some adjustments to simplify the legibility:
\begin{itemize}
    \item Renaming functions whenever they are internal methods or generic auxiliary methods called by different functions.
    \item Externalizing mocking functions that help create fake models and data for unit testing.
    \item Homogenize the code's structure, field types, to help with readability and keeping a standard order along the backend and frontend.
\end{itemize}

\vspace{0.5cm}
\noindent Another very important part of these updating procedure was to adapt the current unit tests to make sure that the original code still behaves as expected, and creating new ones to check that the changes calculates the factor scores properly.

\subsection{Designing new Jobs}

Once all tests have passed and the team had approved the changes, the next step is to create scripts —from now on, \textit{jobs}—to populate the current DB with the historic factor scores while maintaining the servers performance in normal levels. For this we had to update a basic method, \textit{bulk\_create\_or\_update},  used many times through the whole backend and add a new conditioning so it can handle conflicts depending if the models are empty or not. After reading the official Django documentation, we saw that in current versions they had included new fields to its original method "\textit{bulk\_create}", so we had to update Django and make sure it didn't introduce any unwanted issues.

\vspace{0.5cm}
\noindent We decided to create three different jobs for enqueue them and being able to separate the calculations, this way we can restart them in case something broke during the process. And when choosing how to approach the server management, we used the following hierarchy:

\begin{enumerate}
    \item Seven workers for primary daily jobs from different API data integration
    \item One dedicated worker for guaranteeing email sendings, and 6 prioritized shared workers
    \item Four shared workers with lower priority, except for 1 which is set higher than email.
\end{enumerate}

\vspace{0.5cm}
\noindent A crucial part of this code was replicability, equal distribution through different sectors, and mostly to assure that the companies existed during enough time for the factor scores to calculate the 10 years averages.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/tweenvest/Historical Jobs First Iteration.png}
    \caption{Historical Jobs First Iteration}
    \label{fig:historical_jobs_first_iteration}
\end{figure}

\noindent During the first small test it worked in local, but when tested with the production DB we started to see important issues with how the functions where designed due to the large amount of data being processed:
\begin{itemize}
    \item Passing incorrect arguments between jobs—such as entire lists of stock objects—led to the REDIS database reaching its maximum storage capacity.
    \item Jobs ran out of time.
\end{itemize}


\noindent We then changed the day range logic to enqueue a \textit{calculate\_index\_scores\_task} for each day, so the job didn't have to calculate all of the index data for the dates at all at once. Also, we modified the args to only uses models identifiers "id" to reduce the REDIS memory used. The complete implementation of this job can be found in Appendix \ref{app:historical_scores_job}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/tweenvest/Historical Jobs Final Version.png}
    \caption{Historical Jobs Final Version}
    \label{fig:historical_jobs_final_version}
\end{figure}

\subsection{Telemetry Tracing}
Finally we had a functional jobs for calculating the scores, and ran the first calculation for a dataset of 1.500 stocks. After the first jobs finished we quickly realized that they were taking too much time to process, and saw a disproportionate amount of queries to the DB so we started a process to check on where was the bottleneck using telemetry tracing tools.

\vspace{0.5cm}
\noindent To do so, we had to inspect each method and function that is used through the calculations of the factor scores. Luckily we had just recently implemented DataDog which is a software that provides monitoring and analytics for applications and infrastructure, offering real-time metrics, event monitoring, and end-to-end tracing.

\vspace{0.5cm}
\noindent To apply the tracer, we simply had to add a specific decorator before each method and set up the environment for being able to trace not only production launched jobs, but also development tests.

\vspace{0.5cm}
\noindent Here's an example of how the tracer was implemented in the quality score calculation:

\begin{lstlisting}[language=Python, caption=Telemetry Tracing Implementation]
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

@tracer.start_as_current_span("compute_quality_score")
def compute_quality_score(
    stock: "Stock",
    index_quality_ratios: Dict[str, IndexRatio],
    calculation_date: date | None,
) -> tuple[dict, dict]:
    ...
    return final_score
\end{lstlisting}

\noindent We clearly saw an excess of queries to the DB when calculating each score, and tried to reduce them by looking for a simple fault in the logic of the queries. Similar issues had been resolved before by adding a \textit{select\_related} in the queries. To clarify what this does, here is the definition with a simple use-case:

\vspace{0.5cm}
\noindent ``Returns a QuerySet that will follow foreign-key relationships, selecting additional related-object data when it executes its query. This is a performance booster which results in a single more complex query but means later use of foreign-key relationships won't require database queries.'' -- \textcite{django2025queryset}.

\vspace{0.5cm}
\noindent So in Django, \textit{select\_related()} and \textit{prefetch\_related()} are designed to stop the deluge of database queries that are caused by accessing related objects. \textit{select\_related()} ``follows'' foreign-key relationships, selecting additional related-object data when it executes its query. \textit{prefetch\_related()} does a separate lookup for each relationship and does the ``joining'' in Python.

\vspace{0.5cm}
\noindent One uses \textit{select\_related} when the object that you're going to be selecting is a single object, so \textit{OneToOneField} or a \textit{ForeignKey}.

\vspace{0.5cm}
\noindent But after many attempts, and due to the lack of time left to just calculate the factor scores, we had to leave this "minor issue" unsolved and change the strategy to solve the calculation time. Which led to the realization that we were sharing most of the servers power with the normal usage of the platform, and for the normal calculations times (3-4h) didn't matter because we do them daily after market hours, but that didn't work for calculating 5 or more years at once, even with only calculating one score per month.

\subsection{Dedicated Server Deployment}
\noindent We made the decision to establish a dedicated server for this thesis, equipped with enhanced computational power to facilitate the calculations and data fitting. After thorough research, we opted for Hetzner servers due to their exceptional quality-price ratio.

\vspace{0.5cm}
\noindent Deploying the project in a scalable and reproducible environment involved several structured steps, encompassing server provisioning, secure remote access configuration using SSH, and initializing the containerized application deployment via Docker, with the application codebase managed through GitHub.

\vspace{0.5cm}
\noindent After all of the setup was done, we saw amazing performance improvements going from 15-minute calculation times for index statistics to 8 seconds. Finally, we could launch the factor scores historical calculations, so we created two subsets of 1,500 random companies each with the factor scores calculated for the 1st day of the month for the following periods: 2015-2020 and 2020-2025, which took a total of 3 hours.

\vspace{0.5cm}
\noindent Now that we have filled the DB with the necessary factor scores for the study, we ran a big calculation to get all of the possible factor scores for the +100,000 companies, which took several days.

\subsection{Aggregating the data}

For being able to continue, we need to create the final dataset with all the necessary data for later on doing the analysis and developing the predictive models, so we created a simple algorithm with this logic:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/tweenvest/First Data Aggregation Schema.png}
    \caption{First Data Aggregation Schema}
    \label{fig:first_data_aggregation_schema}
\end{figure}

\noindent Once we had the final csv file we noticed that we had too many empty values for profits, the problem was because many of the days calculated were on the weekend. So there was no price history for them, but that wasn't the only thing to take account. Since the companies belong to multiple countries, the holidays were causing another major loss in the information. To fix this in a general way we implemented an internal method for estimating price histories if it didn't exist for the wanted date. The complete implementation of this data export and price estimation process can be found in Appendix \ref{app:data_export_job}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/tweenvest/Final Data Aggregation Schema.png}
    \caption{Final Data Aggregation Schema}
    \label{fig:final_data_aggregation_schema}
\end{figure}

\noindent This can be done due to the fact that we are only looking for long term profitabilities, so we can \textbf{approximate the price} on a day "x" by interpolating the price on "x-y" and "x+z" weighted by how close they are to the original day "x" with a maximum distance from "x" of 15 days. This reduced significantly the amount of missing values.

\subsection{Datasets Creation}

For this work, we are going to attack the main objective from multiple perspectives. First we are going to make a descriptive analysis to understand the data, and then we will propose different models to see if there are any relationships between the scores factors and long term profitabilities.

\vspace{0.5cm}
\noindent When creating the datasets, we made two different approaches:


\begin{figure}[H]
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/code/descriptive analysis/1st Dataset.png}
        \caption{1st Dataset}
        \label{fig:1st_dataset}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/code/descriptive analysis/2nd Dataset.png}
        \caption{2nd Dataset}
        \label{fig:2nd_dataset}
    \end{minipage}
\end{figure}

\noindent For the 1st one, we calculated the scores for the range 2020-2025 and then related them to the profits obtained until the specified date. So, for example, the date 1-1-2021, we would link the explanatory variables to the 2 years’ profitability obtained until that date: 

\begin{equation}
    Profit(\%) = \frac{P_{1\text{-}1\text{-}2021} - P_{1\text{-}1\text{-}2019} + D_{\text{acc}}}{P_{1\text{-}1\text{-}2019}}
\end{equation}

\noindent Which interprets to how earlier profits have influenced on the explanatory variables. This will give us valuable information later on, but we actually want the opposite. So we created the second dataset where we will be looking at the future profits based on current explanatory variables.


\vspace{0.5cm}
\noindent For the 2nd one, we calculated the scores for the range 2015-2020 and then related them to the profits obtained until the specified date. So, for example, the date 1-1-2021, we would link the explanatory variables to the 2 years’ profitability that will be obtained in the future: 

\begin{equation}
    Profit(\%) = \frac{P_{1\text{-}1\text{-}2023} - P_{1\text{-}1\text{-}2021} + D_{\text{acc}}}{P_{1\text{-}1\text{-}2021}}
\end{equation}

\noindent This approach is what we actually want to use for the predictive models, so we will be using the 2nd dataset for the rest of the analysis.

\section{Descriptive Analysis}

\subsection{Preprocessing the Data}

To begin, we looked at the data structure and saw that about 50\% of growth scores were empty. This turned on many alerts from problems with the algorithm, because compared to the rest of the variables there was a significant differences, their absence was only around 8\%. But after digging into the data we realized that it was due to the fact that many companies stop sending reports or selling but keep existing, so we decided to deleted these companies from our dataset because they aren't behaving as a "normal company", and those companies could create a bias in the models. On the other hand, we set all of NaN values of dividend scores as equal to 0 because Tweenvest's algorithm doesn't give a score if the company doesn't pay dividends, but in the study this data is very valuable to see  it if that score is that related to profitability.

\vspace{0.5cm}
\noindent Once we did those small adjustments on the dataset, we ended up with the following distributions:


\vspace{0.5cm}
\noindent The complete implementation of this preprocessing process can be found in Appendix \ref{app:preprocessing_job}.


\subsection{Correlations}

\subsection{Correlations}

\subsection{Correlations}

% ############################### DISCUSSION #######################
\chapter{Discussion}
% Analysis and interpretation

% ############################### CONCLUSION #######################
\chapter{Conclusion}
% Summary and conclusions

% ############################### BIBLIOGRAPHY #######################
\printbibliography[heading=bibintoc, title=Bibliography]
\label{sec:biblio}
\newpage

% ############################### APPENDICES #######################
\part{Appendices}
\def\thechapter{\Alph{chapter}}
\makeatletter
\renewcommand{\@chapapp}{Appendix}
\makeatother

\chapter{Additional Listings}

\section{Historical Scores Job Implementation}
\label{app:historical_scores_job}

\noindent The following code shows the complete implementation of the historical scores job that was used to populate the database with historical factor scores data:

\begin{lstlisting}[language=Python, caption=Historical Scores Job Implementation]
@job("historical_scores", timeout="3h")
def enqueue_calculate_index_scores_task(
    start_date: datetime.date,
    end_date: datetime.date,
    index_names: list[str] | None = None,
    region_names: list[str] | None = None,
    total_stocks: int | None = None,
    seed: float | None = None,
    daily: bool = False,
):
    logger.info(
        "[H1] Starting enqueue_year_tasks from %s to %s",
        start_date,
        end_date,
    )

    days = get_range_from_dates(
        start_date,
        end_date,
        delta=relativedelta(days=1) if daily else relativedelta(months=1),
    )
    days.reverse()

    if not index_names:
        index_names = list(
            Index.objects.exclude(name__in=[Index.INDEX_ALL, "Other"])
            .only("name")
            .values_list("name", flat=True)
        )
    logger.info("[H1] Enqueuing tasks for %s indexes", len(index_names))

    stocks_per_index = (
        int(total_stocks / len(index_names)) if total_stocks else None
    )

    stock_count = 0
    total_stocks_dict = {}
    for index_name in index_names:
        stock_ids = get_stock_ids_for_index(
            index_name,
            stocks_per_index,
            start_date,
            end_date,
            seed,
            region_names,
        )
        if not stock_ids:
            raise ValueError(
                f"[H1] No stocks found for index '{index_name}' after {start_date}"
            )
        total_stocks_dict[index_name] = stock_ids
        for day in days:
            calculate_index_scores_task.delay(
                index_name=index_name,
                day=day,
                stock_ids=stock_ids,
            )

        stock_count += len(stock_ids)

    logger.info(
        "[H1] Enqueued days %s for stocks:%s .", len(days), stock_count
    )

    return {
        "stocks_processed": stock_count,
        "days": len(days),
        "total_stocks": total_stocks_dict,
    }
\end{lstlisting}

\noindent This job function is responsible for:
\begin{itemize}
    \item Accepting date ranges and configuration parameters for historical score calculation
    \item Generating a list of dates to process (either daily or monthly intervals)
    \item Retrieving stock IDs for each index based on the specified criteria
    \item Enqueuing individual calculation tasks for each day and index combination
    \item Providing detailed logging and return statistics about the processing
\end{itemize}

\section{Data Export Job Implementation}
\label{app:data_export_job}

\noindent The following code shows the complete implementation of the data export job that was used to create the final dataset, including the price estimation for missing values due to weekends and holidays:

\begin{lstlisting}[language=Python, caption=Data Export Job Implementation]
def export_factors_and_pricehistory_task(
    stocks_id: list[int],
    start_date: datetime.date,
    end_date: datetime.date,
    export_name: str,
    daily: bool = False,
):
    """
    Export FactorScore, PriceHistory y rentabilidades a CSV sin cargar todo en memoria.
    """
    start_time = time.perf_counter()
    print("Starting CSV export...")

    periods = {
        "profit_1m": relativedelta(months=1),
        "profit_3m": relativedelta(months=3),
        "profit_6m": relativedelta(months=6),
        "profit_1y": relativedelta(years=1),
        "profit_2y": relativedelta(years=2),
        "profit_5y": relativedelta(years=5),
    }

    factor_fields = [
        "stock__ticker",
        "stock__share_type__company__name",
        "stock__share_type__company__industry__industry_group__sector__name",
        "stock__share_type__company__country__region",
        "date",
        "quality",
        "growth",
        "value",
        "dividend",
    ]
    price_fields = ["market_cap_usd", "volume"]

    days = get_range_from_dates(
        start_date,
        end_date,
        delta=relativedelta(days=1) if daily else relativedelta(months=1),
    )
    days.reverse()

    profitability_fields = list(periods.keys())
    export_fields = factor_fields + price_fields + profitability_fields
    missing_prices = 0
    missing_factors = 0
    fs_path = f"api/factors/data_exports/{export_name}.csv"
    with open(fs_path, mode="w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=export_fields)
        writer.writeheader()

        for stock_id in tqdm(stocks_id, desc="Stocks"):

            price_qs = PriceHistory.objects.filter(
                stock__id=stock_id,
                date__range=(
                    start_date,
                    end_date + relativedelta(years=5),
                ),
            ).values("date", "close_price", "market_cap", "volume", "fx_mult")
            price_lookup_stock = {
                (stock_id, ph["date"]): ph
                for ph in price_qs
                if ph["fx_mult"] is not None
            }

            dividend_qs_stock = DividendHistory.objects.filter(
                stock__id=stock_id,
                ex_dividend_date__range=(start_date, end_date),
            ).values("ex_dividend_date", "adjusted_dividend", "fx_mult")
            dividend_lookup = {
                (stock_id, dh["ex_dividend_date"]): (
                    float(dh["adjusted_dividend"]) * float(dh["fx_mult"])
                )
                for dh in dividend_qs_stock
                if dh["fx_mult"] is not None
            }
            fs_qs_stock = (
                FactorScore.objects.filter(stock_id=stock_id, date__in=days)
                .select_related(
                    "stock__share_type__company__industry__industry_group__sector",
                    "stock__share_type__company__country",
                )
                .values(*factor_fields)
            )

            fs_lookup = {(stock_id, fs["date"]): fs for fs in fs_qs_stock}

            for day in days:
                factor = fs_lookup.get((stock_id, day))
                if not factor:
                    missing_factors += 1
                    continue
                day = factor["date"]
                price_day = day
                while price_day.weekday() >= 5:
                    price_day -= datetime.timedelta(days=1)

                price = price_lookup_stock.get((stock_id, price_day))
                if not price or price["close_price"] is None:
                    price = _calculate_existing_prices(
                        price_day, price_lookup_stock, stock_id
                    )
                    if not price:
                        missing_prices += 1
                        continue

                try:
                    market_cap_usd = (
                        float(price.get("market_cap", 0) or 0)
                        * price["fx_mult"]
                    )
                except Exception:
                    market_cap_usd = None
                try:
                    volume = (
                        float(price.get("volume", 0) or 0) * price["fx_mult"]
                    )
                except Exception:
                    volume = None

                if price["close_price"] is None or price["fx_mult"] is None:
                    continue
                close_now_usd = float(price["close_price"]) * price["fx_mult"]

                profitabilities = {}
                for field, delta in periods.items():
                    future_date = day + delta
                    while future_date.weekday() >= 5:
                        future_date -= datetime.timedelta(days=1)

                    future_price = price_lookup_stock.get(
                        (stock_id, future_date)
                    )
                    if not future_price or future_price["close_price"] is None:
                        future_price = _calculate_existing_prices(
                            future_date, price_lookup_stock, stock_id
                        )
                        if not future_price:
                            profitabilities[field] = None
                            missing_prices += 1
                            continue

                    close_future_usd = float(
                        future_price["close_price"]
                    ) * float(future_price["fx_mult"])
                    if close_future_usd == 0:
                        profitabilities[field] = None
                        missing_prices += 1
                        continue
                    # Dividend sum
                    dividend_sum = sum(
                        v
                        for (s_id, ex_div_date), v in dividend_lookup.items()
                        if s_id == stock_id
                        and day <= ex_div_date < future_date
                    )
                    profitabilities[field] = (
                        close_future_usd + dividend_sum - close_now_usd
                    ) / close_now_usd

                row = {**factor}
                row["market_cap_usd"] = market_cap_usd
                row["volume"] = volume
                row.update(profitabilities)
                writer.writerow(row)

    elapsed = time.perf_counter() - start_time
    print(f"Exportado a {fs_path} en {elapsed:.2f}s")
    print("MISSING PRICES:", missing_prices)
    print("MISSING FACTOR SCORES:", missing_factors)
\end{lstlisting}

\noindent This job function is responsible for:
\begin{itemize}
    \item Exporting factor scores, price history and profitability data to CSV without loading everything into memory
    \item Handling missing price data due to weekends and holidays by estimating prices from nearby dates
    \item Converting all monetary values to USD using appropriate exchange rates
    \item Calculating profitability for different time periods (1m, 3m, 6m, 1y, 2y, 5y)
    \item Including dividend payments in profitability calculations
    \item Tracking and reporting missing data points for both prices and factor scores
\end{itemize}

\end{document}
